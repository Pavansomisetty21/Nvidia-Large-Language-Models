{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0849c55f-2b84-4ffa-b929-b838c3e9caf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Augmented Generation (RAG) is a theoretical framework that combines the use of retrieval and generation models to improve the performance of natural language processing (NLP) tasks. The idea behind RAG is to use a retrieval model to first retrieve relevant documents or passages from a large corpus of text, and then use a generative model to produce a response based on the retrieved information.\n",
      "\n",
      "The retrieval model in RAG is typically a dense vector space model, such as a dense passage retriever (DPR), which is trained to map queries and passages to a shared embedding space. The generative model can be any type of language model, such as a transformer-based model, which is trained to generate text based on a given input sequence.\n",
      "\n",
      " a large corpus of text, the generative model is able to generate more accurate and informative responses, even when the input context is sparse or ambiguous. dialogue generation. By retrieving relevant information from\n",
      "\n",
      " RAG is that it allows for the use of a much larger corpus of text than would be possible with traditional generative models. This is because the retrieval model is able to efficiently identify and retrieve only the most relevant passages from the corpus, reducing the amount of text that needs to be processed by the generative model.\n",
      "\n",
      " Augmented Generation is a promising theoretical framework that combines the strengths of retrieval and generative models to improve the performance of NLP tasks. By enabling more accurate and informative responses, RAG has the potential to enhance a wide range of natural language applications, from virtual assistants and chatbots to intelligent search engines and knowledge graphs."
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "client = ChatNVIDIA(\n",
    "  model=\"mistralai/mixtral-8x7b-instruct-v0.1\",\n",
    "  api_key=\"api key\", \n",
    "  temperature=0.5,\n",
    "  top_p=1,\n",
    "  max_tokens=1024,\n",
    ")\n",
    "\n",
    "for chunk in client.stream([{\"role\":\"user\",\"content\":\"tell me about Retrieval Augmented Generation theory\"}]): \n",
    "  print(chunk.content, end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0d238-ce9e-4daf-95ee-8899fec2a359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
